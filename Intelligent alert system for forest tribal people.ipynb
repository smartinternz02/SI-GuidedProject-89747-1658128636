{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the dataset from the below link\n",
    "https://www.kaggle.com/arbethi/wild-animal-detection-and-alerting-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import ImageDataGenerator Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in d:\\intern\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: packaging in d:\\intern\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\intern\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in d:\\intern\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: setuptools in d:\\intern\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\intern\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\intern\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\intern\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\intern\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\intern\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\intern\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\intern\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\intern\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\intern\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\intern\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\intern\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\intern\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\intern\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\intern\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\intern\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\intern\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\intern\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\intern\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=adf658f762ea7efcd85a7067f4ea194be6e59784ece2c1a18fd82c72bf472e44\n",
      "  Stored in directory: c:\\users\\dell6\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built termcolor\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.2.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.6 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Import ImageDataGenerator Library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Import ImageDataGenerator Library\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Define the parameters /arguments for ImageDataGenerator class\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,\n",
    "        rotation_range=180,zoom_range=0.2,horizontal_flip=True)\n",
    "\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying ImageDataGenerator functionality to trainset and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 993 images belonging to 3 classes.\n",
      "Found 353 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Applying ImageDataGenerator functionality to trainset\n",
    "x_train=train_datagen.flow_from_directory(\n",
    "    directory= r\"D:\\Intelligent-alert-system-for-forest-tribal-people-main\\Dataset\\dataset\\dataset\\train_set\",\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "#Applying ImageDataGenerator functionality to test set\n",
    "x_test=test_datagen.flow_from_directory(\n",
    "    directory= r\"D:\\Intelligent-alert-system-for-forest-tribal-people-main\\Dataset\\dataset\\dataset\\test_set\",\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the model building libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the model building libraries'''\n",
    "#to define linear initializations import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "#To add layers import Dense\n",
    "from tensorflow.keras.layers import Dense\n",
    "# to create a convolution kernel import Convolution2D\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "# Adding Max pooling Layer\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "# Adding Flatten Layer\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model\n",
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding CNN layers\n",
    "model.add(Convolution2D(32,(3,3),input_shape=(64,64,3),\n",
    "                        activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Max pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Max pooling Layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Flatten Layer\n",
    "model.add(Flatten()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Hidden Layers\n",
    "model.add(Dense(activation='relu',units=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 2nd hidden layer\n",
    "model.add(Dense(activation='relu',units=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 3rd hidden layer\n",
    "model.add(Dense(activation='relu',units=60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding output layer\n",
    "model.add(Dense(activation='softmax',units=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the learning process or compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the learning process\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farhe\\AppData\\Local\\Temp\\ipykernel_14600\\2366067925.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(x_train,steps_per_epoch=31,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 4s 108ms/step - loss: 1.2637 - accuracy: 0.3788 - val_loss: 1.4558 - val_accuracy: 0.1250\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 8s 260ms/step - loss: 1.0561 - accuracy: 0.4131 - val_loss: 1.1059 - val_accuracy: 0.4034\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.9938 - accuracy: 0.4797 - val_loss: 1.0610 - val_accuracy: 0.4801\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 8s 273ms/step - loss: 0.9342 - accuracy: 0.5349 - val_loss: 1.0615 - val_accuracy: 0.4886\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 0.8745 - accuracy: 0.5578 - val_loss: 0.9479 - val_accuracy: 0.5398\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 8s 273ms/step - loss: 0.8674 - accuracy: 0.5588 - val_loss: 1.0371 - val_accuracy: 0.5597\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.8427 - accuracy: 0.5921 - val_loss: 0.9818 - val_accuracy: 0.5398\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 8s 274ms/step - loss: 0.7902 - accuracy: 0.6401 - val_loss: 0.9373 - val_accuracy: 0.5767\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 9s 273ms/step - loss: 0.9872 - accuracy: 0.5307 - val_loss: 1.1513 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 1.0027 - accuracy: 0.4953 - val_loss: 1.1414 - val_accuracy: 0.5455\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.9393 - accuracy: 0.5525 - val_loss: 1.2492 - val_accuracy: 0.4886\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.8971 - accuracy: 0.5650 - val_loss: 1.0216 - val_accuracy: 0.5966\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 9s 282ms/step - loss: 0.9049 - accuracy: 0.6108 - val_loss: 1.0616 - val_accuracy: 0.5682\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.8418 - accuracy: 0.5952 - val_loss: 1.0949 - val_accuracy: 0.5994\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 8s 266ms/step - loss: 0.7503 - accuracy: 0.6712 - val_loss: 1.4256 - val_accuracy: 0.4915\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.9508 - accuracy: 0.5734 - val_loss: 0.9990 - val_accuracy: 0.5455\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 0.8382 - accuracy: 0.6025 - val_loss: 1.1994 - val_accuracy: 0.5739\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 8s 264ms/step - loss: 0.7608 - accuracy: 0.6556 - val_loss: 1.2074 - val_accuracy: 0.5909\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.7452 - accuracy: 0.6701 - val_loss: 1.0447 - val_accuracy: 0.6222\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.6890 - accuracy: 0.6878 - val_loss: 1.0817 - val_accuracy: 0.6335\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 0.7044 - accuracy: 0.6774 - val_loss: 1.2164 - val_accuracy: 0.5909\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 9s 280ms/step - loss: 0.7038 - accuracy: 0.6889 - val_loss: 1.1557 - val_accuracy: 0.6222\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.6746 - accuracy: 0.7034 - val_loss: 1.0387 - val_accuracy: 0.5938\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 9s 276ms/step - loss: 0.6937 - accuracy: 0.6982 - val_loss: 1.2189 - val_accuracy: 0.5540\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 9s 279ms/step - loss: 0.6684 - accuracy: 0.7128 - val_loss: 1.0405 - val_accuracy: 0.6136\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.6302 - accuracy: 0.7190 - val_loss: 1.0099 - val_accuracy: 0.6364\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 0.6226 - accuracy: 0.7159 - val_loss: 0.9230 - val_accuracy: 0.6648\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.5998 - accuracy: 0.7430 - val_loss: 1.0179 - val_accuracy: 0.6506\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 0.5864 - accuracy: 0.7326 - val_loss: 0.8576 - val_accuracy: 0.6761\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 8s 273ms/step - loss: 0.5807 - accuracy: 0.7555 - val_loss: 1.0847 - val_accuracy: 0.6477\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.5533 - accuracy: 0.7638 - val_loss: 1.2137 - val_accuracy: 0.6193\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.6008 - accuracy: 0.7388 - val_loss: 0.9177 - val_accuracy: 0.6506\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 8s 276ms/step - loss: 0.5453 - accuracy: 0.7513 - val_loss: 1.2114 - val_accuracy: 0.6080\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.7078 - accuracy: 0.6878 - val_loss: 0.9226 - val_accuracy: 0.6335\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.6131 - accuracy: 0.7336 - val_loss: 0.8779 - val_accuracy: 0.6648\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 0.5691 - accuracy: 0.7596 - val_loss: 1.0310 - val_accuracy: 0.6335\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 0.5219 - accuracy: 0.7742 - val_loss: 0.9301 - val_accuracy: 0.6989\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 9s 287ms/step - loss: 0.5842 - accuracy: 0.7336 - val_loss: 1.0567 - val_accuracy: 0.6307\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 0.5815 - accuracy: 0.7211 - val_loss: 0.9444 - val_accuracy: 0.6676\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 0.5539 - accuracy: 0.7544 - val_loss: 0.9456 - val_accuracy: 0.6619\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.5005 - accuracy: 0.7732 - val_loss: 1.1095 - val_accuracy: 0.6534\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.4895 - accuracy: 0.7732 - val_loss: 1.1637 - val_accuracy: 0.6335\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 8s 270ms/step - loss: 0.4786 - accuracy: 0.7908 - val_loss: 1.3274 - val_accuracy: 0.6108\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 8s 271ms/step - loss: 0.5274 - accuracy: 0.7690 - val_loss: 1.0625 - val_accuracy: 0.6364\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.5326 - accuracy: 0.7804 - val_loss: 1.1715 - val_accuracy: 0.6136\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 8s 268ms/step - loss: 0.5283 - accuracy: 0.7690 - val_loss: 0.9590 - val_accuracy: 0.6619\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 8s 272ms/step - loss: 0.4862 - accuracy: 0.7836 - val_loss: 0.9122 - val_accuracy: 0.6847\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 8s 269ms/step - loss: 0.4825 - accuracy: 0.7919 - val_loss: 1.0816 - val_accuracy: 0.6648\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 9s 275ms/step - loss: 0.4683 - accuracy: 0.8004 - val_loss: 0.9526 - val_accuracy: 0.6847\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 8s 267ms/step - loss: 0.5071 - accuracy: 0.7742 - val_loss: 1.0293 - val_accuracy: 0.6307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aff4cfc430>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit_generator(x_train,steps_per_epoch=31,\n",
    "                    epochs=50,\n",
    "                    validation_data=x_test,\n",
    "                    validation_steps=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model \n",
    "model.save('alert.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Human': 0, 'domestic': 1, 'wild': 2}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 204ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#import numpy library\n",
    "import numpy as np\n",
    "#import load_model method to load our saved model\n",
    "from tensorflow.keras.models import load_model\n",
    "#import image from keras.preprocessing\n",
    "from tensorflow.keras.preprocessing import image\n",
    "#loading our saved model file\n",
    "model = load_model(\"alert.h5\")\n",
    "img = image.load_img(r\"D:\\Intelligent-alert-system-for-forest-tribal-people-main\\Dataset\\dataset\\dataset\\train_set\\domestic\\domestic (283).jpg\",\n",
    "                     target_size=(64,64))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "#expanding the shape of image to 4 dimensions\n",
    "x = np.expand_dims(x,axis=0)\n",
    "pred = np.argmax(model.predict(x))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video streaming and alerting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#import opencv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#import numpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#import opencv\n",
    "import cv2\n",
    "#import numpy\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image \n",
    "from tensorflow.keras.models  import load_model\n",
    "#import Client from twilio API\n",
    "from twilio.rest import Client\n",
    "#import playsound package\n",
    "from playsound import playsound\n",
    "\n",
    "#Load saved model file using load_model method\n",
    "model = load_model('alert.h5')\n",
    "#To read webcam\n",
    "video = cv2.VideoCapture(0)\n",
    "#Type of classes or names of the labels that we considered\n",
    "name = ['Human','Domestic', 'Wild']\n",
    "#To execute the program repeatedly using while loop   \n",
    "while(1):\n",
    "    success, frame = video.read()\n",
    "    cv2.imwrite(\"image.jpg\",frame)\n",
    "    img = image.load_img(\"image.jpg\",target_size = (64,64))\n",
    "    x  = image.img_to_array(img)\n",
    "    x = np.expand_dims(x,axis = 0)\n",
    "    pred = np.argmax(model.predict(x))\n",
    "    p = pred\n",
    "    print(pred)\n",
    "    cv2.putText(frame, \"predicted  class = \"+str(name[p]), (100,100), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "    \n",
    "    pred = np.argmax(model.predict(x))\n",
    "    if pred==2:\n",
    "        #twilio account ssid\n",
    "        account_sid = 'ACf4a280f87ab1636d49e847b2b9fe1a81'\n",
    "        #twilo account authentication toke\n",
    "        auth_token = '7578c8622edbfa25c4222958a78551c1'\n",
    "        client = Client(account_sid, auth_token)\n",
    "\n",
    "        message = client.messages \\\n",
    "        .create(\n",
    "         body='Danger!. Wild animal is detected, stay alert',\n",
    "         from_='+15807066854', #the free number of twilio\n",
    "         to='+919398218249')\n",
    "        print(message.sid)\n",
    "        print('Danger!!')\n",
    "        print('Animal Detected')\n",
    "        print ('SMS sent!')\n",
    "        #playsound(r\"D:\\message_tone.mp3\")\n",
    "        #break\n",
    "    else:\n",
    "        print(\"No Danger\")\n",
    "       #break\n",
    "    cv2.imshow(\"image\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('a'): \n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import cv2\\n#import facevec\\nimport numpy as np\\nfrom keras.preprocessing import image \\nfrom keras.models  import load_model\\n\\nmodel = load_model(\\'alert.h5\\') \\nvideo = cv2.VideoCapture(0)\\nname = [\"Human\",\"Wild aniaml\",\"otimher\"]\\n    \\nwhile(1):\\n    success, frame = video.read()\\n    cv2.imwrite(\"image.jpg\",frame)\\n    img = image.load_img(\"image.jpg\",target_size = (64,64))\\n    x  = image.img_to_array(img)\\n    x = np.expand_dims(x,axis = 0)\\n    pred = model.predict_classes(x)\\n    p = pred[0]\\n    print(pred)\\n    cv2.putText(frame, \"predicted  class = \"+str(name[p]), (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\\n    cv2.imshow(\"image\",frame)\\n    if cv2.waitKey(1) & 0xFF == ord(\\'a\\'): \\n        break\\n\\nvideo.release()\\ncv2.destroyAllWindows()'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''import cv2\n",
    "#import facevec\n",
    "import numpy as np\n",
    "from keras.preprocessing import image \n",
    "from keras.models  import load_model\n",
    "\n",
    "model = load_model('alert.h5') \n",
    "video = cv2.VideoCapture(0)\n",
    "name = [\"Human\",\"Wild aniaml\",\"otimher\"]\n",
    "    \n",
    "while(1):\n",
    "    success, frame = video.read()\n",
    "    cv2.imwrite(\"image.jpg\",frame)\n",
    "    img = image.load_img(\"image.jpg\",target_size = (64,64))\n",
    "    x  = image.img_to_array(img)\n",
    "    x = np.expand_dims(x,axis = 0)\n",
    "    pred = model.predict_classes(x)\n",
    "    p = pred[0]\n",
    "    print(pred)\n",
    "    cv2.putText(frame, \"predicted  class = \"+str(name[p]), (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1)\n",
    "    cv2.imshow(\"image\",frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('a'): \n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
